{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading_libraries..\n"
     ]
    }
   ],
   "source": [
    "#import libraies\n",
    "print('loading_libraries..')\n",
    "import tensorflow as tf\n",
    "import random \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n",
    "epoch          = 10\n",
    "data_set_lenth = 100\n",
    "batch_size     =5\n",
    "iteration      =int(data_set_len//batch_size)\n",
    "\n",
    "#data_loading\n",
    "print('loading_data_files')\n",
    "\n",
    "pos=np.load('positive_reviews_imbd.npy')\n",
    "neg=np.load('negative_reviews_imbd.npy')\n",
    "\n",
    "data_x  =np.load('word_embedding_lstm.npy')\n",
    "words_y =np.load('words_list_lstm.npy')\n",
    "\n",
    "mixed_data=[]\n",
    "for i in pos:\n",
    "    mixed_data.append((1,i))\n",
    "for j in neg[:100]:\n",
    "    mixed_data.append((0,j))\n",
    "\n",
    "random.shuffle(mixed_data)\n",
    "\n",
    "def padding_data(data_):\n",
    "    input_x_data = []\n",
    "    output_y_data = []\n",
    "    max_value = max([len(j) for i, j in data_])\n",
    "\n",
    "    final_data = [(j, i + [0] * (max_value - len(i))) \n",
    "                  if len(i) < max_value else (j, i) \n",
    "                  for j, i in data_]\n",
    "    \n",
    "    for i, j in final_data:\n",
    "        input_x_data.append(j)\n",
    "        output_y_data.append(i)\n",
    "\n",
    "    return {'input': input_x_data, 'output': output_y_data}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6933215 0.41666666 (12, 2) (12, 13, 102) (12, 13, 13) (12, 13) (12, 2) (12, 26)\n"
     ]
    }
   ],
   "source": [
    "class Lstm_sentiment_network(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,vocab_size,word_embedding_dim,num_uni,labels):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        sentence  = tf.placeholder(name='input_sentence',shape=[None,None],dtype=tf.int32)\n",
    "        sentiment_= tf.placeholder(name='sentiment',shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        self.placeholders = {'sentence':sentence,'sentiment':sentiment_}\n",
    "        \n",
    "        word_embedding = tf.get_variable(name='word_embedding_',\n",
    "                                         shape=[vocab_size,word_embedding_dim],\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        embedding_lookup = tf.nn.embedding_lookup(word_embedding,sentence)\n",
    "        \n",
    "        sequence_leng = tf.count_nonzero(sentence,axis=-1)\n",
    "        \n",
    "        with tf.variable_scope('forward'):\n",
    "            fr_cell = tf.contrib.rnn.LSTMCell(num_units=num_uni)\n",
    "            dropout_fr = tf.contrib.rnn.DropoutWrapper(fr_cell)\n",
    "            \n",
    "        with tf.variable_scope('backward'):\n",
    "            bw_cell = tf.contrib.rnn.LSTMCell(num_units=num_uni)\n",
    "            dropout_bw = tf.contrib.rnn.DropoutWrapper(bw_cell)\n",
    "            \n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            model,last_state = tf.nn.bidirectional_dynamic_rnn(dropout_fr,\n",
    "                                                               dropout_bw,\n",
    "                                                               inputs=embedding_lookup,\n",
    "                                                               sequence_length=sequence_leng,\n",
    "                                                               dtype=tf.float32)\n",
    "            \n",
    "        concat_output = tf.concat([last_state[0].c,last_state[1].c],axis=-1)\n",
    "        \n",
    "        fc_layer = tf.get_variable(name='fully_connected',\n",
    "                                   shape=[2*num_uni,labels],\n",
    "                                   dtype=tf.float32,\n",
    "                                   initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        bias    = tf.get_variable(name='bias',\n",
    "                                   shape=[labels],\n",
    "                                   dtype=tf.float32,\n",
    "                                   initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        projection = tf.add(tf.matmul(concat_output,fc_layer),bias)\n",
    "        \n",
    "        #prediction\n",
    "        probability = tf.nn.softmax(projection)\n",
    "        prediction  = tf.argmax(probability,axis=-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=projection,labels=sentiment_)\n",
    "        \n",
    "        cost = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        #accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(prediction,tf.int32),sentiment_),tf.float32))\n",
    "        \n",
    "        \n",
    "        self.output = {'loss':cost,'accuracy':accuracy,'logits': projection,'check1':embedding_lookup,'check2':model,'check3':last_state,'projection':projection,'con':concat_output}\n",
    "        self.train = tf.train.AdamOptimizer().minimize(cost)\n",
    "        \n",
    "        \n",
    "def model_execute(model):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        model_out,train = sess.run([model.output,model.train],feed_dict={model.placeholders['sentence']:np.random.randint(0,10,[12,13]),model.placeholders['sentiment']:np.random.randint(0,2,[12,])})\n",
    "        \n",
    "        \n",
    "        print(model_out['loss'],model_out['accuracy'],model_out['logits'].shape,model_out['check1'].shape,model_out['check2'][0].shape,model_out['check3'][0].c.shape,model_out['projection'].shape,model_out['con'].shape)\n",
    "        \n",
    "        \n",
    "if __name__=='__main__':\n",
    "    \n",
    "    model_out_ = Lstm_sentiment_network(121,102,13,2)\n",
    "    model_execute(model_out_)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
