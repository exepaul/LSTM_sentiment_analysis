{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 417195\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {}\n",
    "with open('numberbatch-en-17.06.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import unicodedata\n",
    "import time\n",
    "import os\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "punctuation = dict.fromkeys([i for i in range(sys.maxunicode)\n",
    "                                 if unicodedata.category(chr(i)).startswith('P')])\n",
    "\n",
    "with open('data_pita.pkl','rb') as f:\n",
    "    data_set = pk.load(f)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "with open('vocab_pita.pkl','rb') as f:\n",
    "    vocab_ = pk.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set =[]\n",
    "# vocab_ =  []\n",
    "\n",
    "# for i,j in tqdm(open_data.items()):\n",
    "#     kl=[i.lower() for i in nltk.word_tokenize(i.translate(punctuation))]\n",
    "#     data_set.append((kl,j))\n",
    "#     vocab_.extend(kl)\n",
    "\n",
    "    \n",
    "# # with open('vocab_soha.pkl','wb') as f:\n",
    "# #     pk.dump(list(set(vocab_)),f)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# # with open('data_soha.pkl','wb') as f:\n",
    "# #     pk.dump(data_set,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_o = ['PAD'] + list(set(vocab_))\n",
    "vocab_to_int = {j:i for i,j in enumerate(vocab_o)}\n",
    "int_to_vocab ={k:m for m,k in vocab_to_int.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7086/7086 [00:00<00:00, 222669.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3995\n",
      "3091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "encoded_data =[]\n",
    "for i in tqdm(data_set):\n",
    "        encoded_data.append(([vocab_to_int[m] for m in i[0] if m in vocab_to_int],i[1]))\n",
    "        \n",
    "random.shuffle(encoded_data)\n",
    "\n",
    "test_data = encoded_data[:100]\n",
    "train_data = encoded_data[100:]\n",
    "\n",
    "np.save('testt',test_data)\n",
    "\n",
    "\n",
    "targ =[]\n",
    "for i in encoded_data:\n",
    "    targ.append(i[1])\n",
    "    \n",
    "print(targ.count(1))\n",
    "print(targ.count(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_f =[0, 1,]\n",
    "\n",
    "real_ca = {0:'negative',1:'postive'}\n",
    "\n",
    "\n",
    "def get_batch(i,batch_size,test=None):\n",
    "    if test==0:\n",
    "        \n",
    "        encoded_data_r = test_data[i*batch_size:(i+1)*batch_size]\n",
    "    else:\n",
    "        encoded_data_r = encoded_data[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        \n",
    "\n",
    "    sentence=[]\n",
    "    sentiment =[]\n",
    "    for i in encoded_data_r:\n",
    "        sentence.append(i[0])\n",
    "        sentiment.append(categories_f.index(i[1]))\n",
    "        \n",
    "    max_len = max([len(i) for i in  sentence])\n",
    "    \n",
    "    pad_data = [m + [0]*(max_len -len(m)) if len(m)<max_len else m for m in sentence]\n",
    "    \n",
    "    return np.array(pad_data) , np.array(sentiment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"Monk\"\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class lstm_Attention_with_Pos(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,vocab_size,num_cell,dropout_value,category_size):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #placeholders\n",
    "        sentence = tf.placeholder(name='input_data',shape=[None,None],dtype=tf.int32)\n",
    "#         pos_     = tf.placeholder(name='pos',shape=[None,None],dtype=tf.int32)\n",
    "        intents  = tf.placeholder(name='labels',shape=[None],dtype=tf.int32)\n",
    "        mode =  tf.placeholder(name='dropout',shape=(),dtype=tf.int32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.placeholder = {\n",
    "            \n",
    "                            'input_d':sentence,\n",
    "                            'labels':intents,\n",
    "#                              'pos':pos_,\n",
    "                            'dropout':mode\n",
    "                            }\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#         embedding_matrix = tf.get_variable(name='word_embedding',\n",
    "#                                            shape=[vocab_size,embedding_dim],\n",
    "#                                            dtype=tf.float32,\n",
    "#                                            initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        final_input = tf.nn.embedding_lookup(word_embedding_matrix,sentence)\n",
    "        \n",
    "        dropout = tf.cond(\n",
    "                          tf.equal(mode,0),\n",
    "                          lambda : dropout_value, \n",
    "                          lambda : 0.\n",
    "                         )\n",
    "        \n",
    "#         pos_embedding = tf.get_variable(name='pos_embedding',\n",
    "#                                            shape=[pos_vocab_size,pdim],\n",
    "#                                            dtype=tf.float32,\n",
    "#                                            initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        \n",
    "#         pos_lookup = tf.nn.embedding_lookup(pos_embedding, pos_ )\n",
    "        \n",
    "#         final_input = tf.concat([embedd_word2vec,pos_lookup],axis=-1)\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sequence_length = tf.count_nonzero(sentence,axis=-1)\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('forward_cell'):\n",
    "                lstm_cell_fw = tf.contrib.rnn.LSTMCell(num_cell,forget_bias=1.0)\n",
    "                dropout_wrapper_fw = tf.contrib.rnn.DropoutWrapper(lstm_cell_fw,output_keep_prob=1. - dropout_value)\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('backward_cell'):\n",
    "            lstm_cell_bw = tf.contrib.rnn.LSTMCell(num_cell,forget_bias=1.0)\n",
    "            dropout_wrapper_bw = tf.contrib.rnn.DropoutWrapper(lstm_cell_bw,output_keep_prob=1.- dropout_value)\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('bi_lstm') as scope:\n",
    "            output , last_state     = tf.nn.bidirectional_dynamic_rnn(dropout_wrapper_fw,\n",
    "                                                      dropout_wrapper_bw,\n",
    "                                                      final_input,\n",
    "                                                      sequence_length=sequence_length,\n",
    "                                                      dtype=tf.float32)\n",
    "        \n",
    "        logits = tf.concat(output,2)\n",
    "        #ex . 12 x 10 x 24\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Attention_layer\n",
    "        \n",
    "        \n",
    "        #ex : 12x10x24 . ===> 120 x 24\n",
    "        input_reshape = tf.reshape(logits,[-1,num_cell*2])\n",
    "        \n",
    "        \n",
    "        #num_cell = 12\n",
    "        #ex : # 24 x 1\n",
    "        attention_size = tf.get_variable(name='attention_size',\n",
    "                                         shape=[2*num_cell,1],\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        # bias 1\n",
    "        bias          = tf.get_variable(name='bias',shape=[1],\n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        \n",
    "        #projection without activation \n",
    "        #ex : 120x24 matmul 24x 1 ==> 120x1\n",
    "        attention_projection = tf.add(tf.matmul(input_reshape,attention_size),bias)\n",
    "        \n",
    "        \n",
    "        #reshape . 120x1 ==> 12x10x1 (shape of input )\n",
    "        output_reshape = tf.reshape(attention_projection,[tf.shape(sentence)[0],tf.shape(sentence)[1],-1])\n",
    "        \n",
    "        #softmax over logits 12x10x1\n",
    "        attention_output = tf.nn.softmax(output_reshape,dim=1)\n",
    "        \n",
    "        \n",
    "        #reshape as input 12x10\n",
    "        attention_visualize = tf.reshape(attention_output,\n",
    "                                         [tf.shape(sentence)[0],\n",
    "                                          tf.shape(sentence)[1]],\n",
    "                                         name='Plot')\n",
    "        \n",
    "        \n",
    "        # 12x10x1 multiply 12x10x24  == > 12x10x24\n",
    "        attention_projection_output = tf.multiply(attention_output,logits)\n",
    "        \n",
    "        #reduce across time 120x10x24 ==> 12x24\n",
    "        Final_output = tf.reduce_sum(attention_projection_output,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #fully_connected_layer\n",
    "        \n",
    "        f_c = tf.get_variable(name='f_c_layer',\n",
    "                              shape=[2*num_cell,category_size],\n",
    "                              dtype=tf.float32,\n",
    "                              initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        \n",
    "        f_bias = tf.get_variable(name='fc_bias',\n",
    "                                 shape=[category_size],\n",
    "                                dtype=tf.float32,\n",
    "                                 initializer=tf.random_uniform_initializer(-0.01,0.01))\n",
    "        \n",
    "        output_projection = tf.add(tf.matmul(Final_output,f_c),f_bias)\n",
    "        \n",
    "        \n",
    "        \n",
    "        probability_dist  = tf.nn.softmax(output_projection,name='pred')\n",
    "        \n",
    "        prediction  = tf.argmax(probability_dist,axis=-1)\n",
    "        \n",
    "        \n",
    "        #as_usual_stuff_\n",
    "        cross_entropy  = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=intents,logits=output_projection)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        accuracy_calculation = tf.reduce_mean(tf.cast(tf.equal(tf.cast(prediction,tf.int32),intents),tf.float32))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.output = {  'loss':loss,\n",
    "                         'accuracy':accuracy_calculation,\n",
    "                         'prediction':prediction,\n",
    "                         'probai':probability_dist,\n",
    "                         'logits':output_projection,\n",
    "                         'attention_visualize':attention_visualize\n",
    "            \n",
    "                       }\n",
    "        \n",
    "        \n",
    "        # this guy is hero of this movie\n",
    "        self.train = tf.train.AdamOptimizer().minimize(loss)\n",
    "        \n",
    "\n",
    "# def execute_model(model):\n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "        \n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "#         for i in range(100):\n",
    "#             outp,train = sess.run([model.output,model.train],feed_dict={model.placeholders['input_d']:np.random.randint(0,3,[12,10]),\n",
    "#                                                 model.placeholders['labels']:np.random.randint(0,2,[12,]),\n",
    "                                                \n",
    "# #                                                 model.placeholders['pos']: np.random.randint(0,3,[12,10]),\n",
    "#                                                 model.placeholders['dropout']:0,\n",
    "#                                                                        })\n",
    "#             print(outp['loss'],outp['accuracy'],outp['prediction'].shape,outp['attention_visualize'].shape)\n",
    "        \n",
    "        \n",
    "# if __name__ == '__main__':\n",
    "    \n",
    "#     model = lstm_Attention_with_Pos(130,100,12,0.5,120,22,12)\n",
    "#     execute_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 70\n",
      "{'epoch': 0, 'iteration': 0, 'training_loss': 0.69257826, 'training_accuracy': 0.6}\n",
      "{'epoch': 0, 'iteration': 1, 'training_loss': 0.69146955, 'training_accuracy': 0.6}\n",
      "{'epoch': 0, 'iteration': 2, 'training_loss': 0.69036394, 'training_accuracy': 0.6}\n",
      "{'epoch': 0, 'iteration': 3, 'training_loss': 0.6890548, 'training_accuracy': 0.6}\n",
      "{'epoch': 0, 'iteration': 4, 'training_loss': 0.6873149, 'training_accuracy': 0.62}\n",
      "{'epoch': 0, 'iteration': 5, 'training_loss': 0.68481845, 'training_accuracy': 0.62}\n",
      "{'epoch': 0, 'iteration': 6, 'training_loss': 0.68165475, 'training_accuracy': 0.62}\n",
      "{'epoch': 0, 'iteration': 7, 'training_loss': 0.6776003, 'training_accuracy': 0.62}\n",
      "{'epoch': 0, 'iteration': 8, 'training_loss': 0.67339295, 'training_accuracy': 0.62}\n",
      "{'epoch': 0, 'iteration': 9, 'training_loss': 0.6692241, 'training_accuracy': 0.63}\n",
      "{'epoch': 0, 'iteration': 10, 'training_loss': 0.66206115, 'training_accuracy': 0.63}\n",
      "{'epoch': 0, 'iteration': 11, 'training_loss': 0.65178114, 'training_accuracy': 0.63}\n",
      "{'epoch': 0, 'iteration': 12, 'training_loss': 0.6413488, 'training_accuracy': 0.63}\n",
      "{'epoch': 0, 'iteration': 13, 'training_loss': 0.6285379, 'training_accuracy': 0.65}\n",
      "{'epoch': 0, 'iteration': 14, 'training_loss': 0.6074681, 'training_accuracy': 0.68}\n",
      "{'epoch': 0, 'iteration': 15, 'training_loss': 0.58225936, 'training_accuracy': 0.68}\n",
      "{'epoch': 0, 'iteration': 16, 'training_loss': 0.5516479, 'training_accuracy': 0.7}\n",
      "{'epoch': 0, 'iteration': 17, 'training_loss': 0.5215794, 'training_accuracy': 0.72}\n",
      "{'epoch': 0, 'iteration': 18, 'training_loss': 0.49244472, 'training_accuracy': 0.77}\n",
      "{'epoch': 0, 'iteration': 19, 'training_loss': 0.46454093, 'training_accuracy': 0.8}\n",
      "{'epoch': 0, 'iteration': 20, 'training_loss': 0.4389627, 'training_accuracy': 0.8}\n",
      "{'epoch': 0, 'iteration': 21, 'training_loss': 0.41136023, 'training_accuracy': 0.83}\n",
      "{'epoch': 0, 'iteration': 22, 'training_loss': 0.39247373, 'training_accuracy': 0.85}\n",
      "{'epoch': 0, 'iteration': 23, 'training_loss': 0.37441754, 'training_accuracy': 0.86}\n",
      "{'epoch': 0, 'iteration': 24, 'training_loss': 0.3549415, 'training_accuracy': 0.87}\n",
      "{'epoch': 0, 'iteration': 25, 'training_loss': 0.32700625, 'training_accuracy': 0.88}\n",
      "{'epoch': 0, 'iteration': 26, 'training_loss': 0.38992837, 'training_accuracy': 0.86}\n",
      "{'epoch': 0, 'iteration': 27, 'training_loss': 0.48144218, 'training_accuracy': 0.84}\n",
      "{'epoch': 0, 'iteration': 28, 'training_loss': 0.46383584, 'training_accuracy': 0.85}\n",
      "{'epoch': 0, 'iteration': 29, 'training_loss': 0.4523736, 'training_accuracy': 0.86}\n",
      "{'epoch': 0, 'iteration': 30, 'training_loss': 0.44748825, 'training_accuracy': 0.87}\n",
      "{'epoch': 0, 'iteration': 31, 'training_loss': 0.42885238, 'training_accuracy': 0.9}\n",
      "{'epoch': 0, 'iteration': 32, 'training_loss': 0.41763264, 'training_accuracy': 0.89}\n",
      "{'epoch': 0, 'iteration': 33, 'training_loss': 0.5363944, 'training_accuracy': 0.89}\n",
      "{'epoch': 0, 'iteration': 34, 'training_loss': 0.50458723, 'training_accuracy': 0.9}\n",
      "{'epoch': 0, 'iteration': 35, 'training_loss': 0.46744874, 'training_accuracy': 0.9}\n",
      "{'epoch': 0, 'iteration': 36, 'training_loss': 0.43105182, 'training_accuracy': 0.91}\n",
      "{'epoch': 0, 'iteration': 37, 'training_loss': 0.39153054, 'training_accuracy': 0.92}\n",
      "{'epoch': 0, 'iteration': 38, 'training_loss': 0.3455435, 'training_accuracy': 0.92}\n",
      "{'epoch': 0, 'iteration': 39, 'training_loss': 0.31599972, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 40, 'training_loss': 0.30508652, 'training_accuracy': 0.93}\n",
      "{'epoch': 0, 'iteration': 41, 'training_loss': 0.3240534, 'training_accuracy': 0.91}\n",
      "{'epoch': 0, 'iteration': 42, 'training_loss': 0.33265954, 'training_accuracy': 0.88}\n",
      "{'epoch': 0, 'iteration': 43, 'training_loss': 0.34222734, 'training_accuracy': 0.87}\n",
      "{'epoch': 0, 'iteration': 44, 'training_loss': 0.31496903, 'training_accuracy': 0.91}\n",
      "{'epoch': 0, 'iteration': 45, 'training_loss': 0.27829742, 'training_accuracy': 0.95}\n",
      "{'epoch': 0, 'iteration': 46, 'training_loss': 0.26760513, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 47, 'training_loss': 0.25329587, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 48, 'training_loss': 0.25465533, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 49, 'training_loss': 0.2531261, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 50, 'training_loss': 0.24425635, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 51, 'training_loss': 0.2469917, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 52, 'training_loss': 0.23411845, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 53, 'training_loss': 0.22618575, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 54, 'training_loss': 0.22360606, 'training_accuracy': 0.95}\n",
      "{'epoch': 0, 'iteration': 55, 'training_loss': 0.21936913, 'training_accuracy': 0.95}\n",
      "{'epoch': 0, 'iteration': 56, 'training_loss': 0.24828243, 'training_accuracy': 0.92}\n",
      "{'epoch': 0, 'iteration': 57, 'training_loss': 0.18655473, 'training_accuracy': 0.95}\n",
      "{'epoch': 0, 'iteration': 58, 'training_loss': 0.18316635, 'training_accuracy': 0.95}\n",
      "{'epoch': 0, 'iteration': 59, 'training_loss': 0.23776639, 'training_accuracy': 0.96}\n",
      "{'epoch': 0, 'iteration': 60, 'training_loss': 0.24595943, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 61, 'training_loss': 0.16566528, 'training_accuracy': 0.95}\n",
      "{'epoch': 0, 'iteration': 62, 'training_loss': 0.3788552, 'training_accuracy': 0.93}\n",
      "{'epoch': 0, 'iteration': 63, 'training_loss': 0.30763856, 'training_accuracy': 0.92}\n",
      "{'epoch': 0, 'iteration': 64, 'training_loss': 0.15964107, 'training_accuracy': 0.95}\n",
      "{'epoch': 0, 'iteration': 65, 'training_loss': 0.23995121, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 66, 'training_loss': 0.23018563, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'iteration': 67, 'training_loss': 0.31553754, 'training_accuracy': 0.95}\n",
      "{'epoch': 0, 'iteration': 68, 'training_loss': 0.32911807, 'training_accuracy': 0.93}\n",
      "{'epoch': 0, 'iteration': 69, 'training_loss': 0.22458568, 'training_accuracy': 0.94}\n",
      "{'epoch': 0, 'test_accuracy': 0.93999994}\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "def evaluate_(model, batch_size=10):\n",
    "\n",
    "    sess = tf.get_default_session()\n",
    "\n",
    "    # batch_data = test_data_in\n",
    "    # batch_labesl = train_labels\n",
    "\n",
    "    iteration = len(test_data) // batch_size\n",
    "\n",
    "    accuracy = []\n",
    "\n",
    "    for i in range(iteration):\n",
    "        input_test,labels = get_batch(i,10,test=0)\n",
    "\n",
    "        network_out = sess.run(model.output, feed_dict={model.placeholder['input_d']: input_test,\n",
    "                                                        model.placeholder['labels']: labels,\n",
    "                                                        model.placeholder['dropout']: 1})\n",
    "\n",
    "        accuracy.append(network_out['accuracy'])\n",
    "    return np.mean(np.array(accuracy))\n",
    "\n",
    "\n",
    "def train_model(model, batch_size=100):\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        iteration = len(encoded_data) // batch_size\n",
    "        print(\"iteration\", iteration)\n",
    "        \n",
    "\n",
    "        for i in range(epoch):\n",
    "\n",
    "            for j in range(iteration):\n",
    "                \n",
    "                input_train,labels = get_batch(i,100)\n",
    "\n",
    "                \n",
    "\n",
    "                network_out, _ = sess.run([model.output, model.train],\n",
    "                                          feed_dict={model.placeholder['input_d']: input_train,\n",
    "                                                     model.placeholder['labels']: labels,\n",
    "                                                     model.placeholder['dropout']: 0})\n",
    "\n",
    "                print({'epoch': i,\n",
    "                       'iteration': j,\n",
    "                       'training_loss': network_out['loss'],\n",
    "                       'training_accuracy': network_out['accuracy']\n",
    "                       })\n",
    "                \n",
    "\n",
    "            print({'epoch': i, 'test_accuracy': evaluate_(model)})\n",
    "            with open('epochandresult', 'a') as f:\n",
    "                f.write(str({'epoch': i, 'test_accuracy': evaluate_(model)}) + '\\n')\n",
    "            os.system('mkdir ' + str(i) + 'epoch')\n",
    "            saver.save(sess, '/Users/monk/Desktop/Word_cloud_model/movies/punjab_grill/haha/lk' + str(i) + 'epoch' + '/' + str(i))\n",
    "            time.sleep(5)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = lstm_Attention_with_Pos(len(vocab_),200,0.5,2)\n",
    "\n",
    "    train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle  as pk\n",
    "import nltk\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "with open('vocab_pita.pkl','rb') as f:\n",
    "    vocab_o = pk.load(f)\n",
    "\n",
    "vocab_to_int = {j:i for i,j in enumerate(vocab_o)}\n",
    "int_to_vocab ={k:m for m,k in vocab_to_int.items()}\n",
    "\n",
    "punctuation=dict.fromkeys([i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P')])\t\n",
    "\t\n",
    "def remove_pun(text):\t\n",
    "    #removing punctuation\t\n",
    "    return [i.lower() for i in nltk.word_tokenize(text.translate(punctuation))]\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess =  tf.Session()\t\n",
    "# saver = tf.train.import_meta_graph('16epoch/16.meta')\t\n",
    "# restore = saver.restore(sess,tf.train.latest_checkpoint('16epoch/'))\t\n",
    "\t\n",
    "# graph=tf.get_default_graph()\t\n",
    "\t\n",
    "# query= graph.get_tensor_by_name(\"input_data:0\")\t\n",
    "# result=graph.get_tensor_by_name(\"pred:0\")\t\n",
    "# plot=graph.get_tensor_by_name(\"Plot:0\")\t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def encode_query(text_):\t\n",
    "#     query=remove_pun(text_)\t\n",
    "#     sequence_data=[]\t\n",
    "#     for i in query:\t\n",
    "\t\n",
    "#         if i in vocab_to_int:\t\n",
    "#             sequence_data.append(vocab_to_int[i])\t\n",
    "\t\n",
    "#     return sequence_data\n",
    "\n",
    "\n",
    "# labels_dict = {0:'neutal',1:'postive',2:'negative'}\n",
    "# categories_f =[0, 1,-1]\n",
    "\n",
    "\n",
    "    \t\n",
    "# def predict(user_query_,sess=sess):\t\n",
    "# #         user_query_p = user_query_.split()\n",
    "#         user_query = user_query_\n",
    "#         feed_dict = {query:[user_query]}\t\n",
    "        \t\n",
    "#         prediction,plot_a = sess.run([result,plot],feed_dict=feed_dict)\t\n",
    "        \n",
    "# #         print(prediction)\n",
    "#         return labels_dict[prediction[0].tolist().index(max(prediction[0].tolist()))]\n",
    "# #         print(list(zip(user_query_p,plot_a[0])))\n",
    "# #         pred=labels_dict[prediction[0].index(max(prediction[0]))]\t\n",
    "# #         print(plot_a,user_query_)\n",
    "# #         return { \t\n",
    "# #                  'Adverse_Event_Flag': pred ,\t\n",
    "# #                  'Probability'       : max(prediction[0])\t\n",
    "# #                }\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong=0\n",
    "for i in test_data:\n",
    "    if predict(i[0])!=labels_dict[categories_f.index(i[1])]:\n",
    "        wrong+=1\n",
    "print(wrong)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
